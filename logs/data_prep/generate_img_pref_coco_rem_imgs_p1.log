nohup: ignoring input
You are using a model of type llava_next to instantiate a model of type llava_next_video. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:02<00:05,  2.69s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:05<00:02,  2.55s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:07<00:00,  2.33s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:07<00:00,  2.40s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Total number of images:  2300
  0%|          | 0/2300 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)
  0%|          | 1/2300 [00:32<20:59:41, 32.88s/it]  0%|          | 2/2300 [00:58<18:15:51, 28.61s/it]  0%|          | 3/2300 [01:21<16:33:53, 25.96s/it]  0%|          | 4/2300 [01:49<17:10:44, 26.94s/it]  0%|          | 5/2300 [02:13<16:31:24, 25.92s/it]  0%|          | 6/2300 [02:36<15:44:11, 24.70s/it]  0%|          | 7/2300 [03:06<16:52:32, 26.49s/it]  0%|          | 8/2300 [03:24<15:10:34, 23.84s/it]  0%|          | 9/2300 [04:14<20:16:44, 31.87s/it]  0%|          | 10/2300 [04:40<19:14:32, 30.25s/it]  0%|          | 11/2300 [05:08<18:44:49, 29.48s/it]  1%|          | 12/2300 [05:40<19:18:25, 30.38s/it]